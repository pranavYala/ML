After importing the necessary libraries and packages, I first loaded in the raw dataset and the list of example skills as two seperate Panda DataFrames. Then, I began pre-processing the two datasets. This included basic changes like making all of the characters lower case, getting rid of any punctuation, dropping any duplicate elements, and getting rid of extra spaces. In addition to this, I also got rid of any stop words using the stop words corpus from nltk, lemmatized all of the words to get rid of any stems and conjugations that would make text classification harder, and also split up each phrase into a list of words if it contained more than 1 word. Now, every phrase was an individual element, with multi-word phrases being a list of the words it consists of. Then, I began the process of training a Word2vec model using the example list of technical skills as my custom corpus. After training the Word2vec model, I started looping through every word of the raw data using nested for loops. Then, I compared each word of the raw data to each word of the technical skill corpus. For each word from the raw dataset, I calculated the distance between its word vector and the word vectors of every word in the corpus. I summed up these distances for each phrase as whole, so if a phrase had more words, it was probable that it would have a much higher sum than a phrase consisting of only one word. I decided to leave this bias towards longer phrases as is, as I noticed that a lot of the longer phrases weren't actually skills but rather just a string of keywords thrown together. The thinking behind this calculation process was that the word vector of a phrase that represents a technical skill would likely be closer to the word vectors represented by the words in the technical skills corpus than the vector of random jargon. Therefore, the smaller the sum of distances for a phrase, the more likely it is that the phrase represents a technical skill. The sum obviously starts at 0 for each phrase, and the threshold I chose for the sum of distances was 1500 (I used a negative scale so for me it was actually -1500). So, any phrase whose collective word vectors sum up to a distance of more than 1500 are deemed by me to probably not be a technical skill. If the sum is within this range, I add that phrase to the list of filtered, technical terms. This approach could definitely be improved on, although it is hard to so with such a small corpus. With a larger, more complete corpus, much better text classification can be done.
